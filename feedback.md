# Assignment Feedback: Week 4: Dimensionality Reduction

**Student:** akashkuteX
**Raw Score:** 47/50 (94.0%)
**Course Points Earned:** 4

---

## Problem Breakdown

### Exercise 2 (10/10 = 100.0%)

**Part ex1-part1** (ex1-part1.code): 4/4 points

_Feedback:_ Great job. You correctly applied t-SNE (with PCA preprocessing, subsampling, normalization) and visualized digits with colors and a legend. Robust handling of params and versions. Minor nit: the title’s perplexity may not match the adjusted value used.

**Part ex1-part2** (ex1-part2.code): 3/3 points

_Feedback:_ Good job. You computed t-SNE embeddings, trained a KNN on them, and reported accuracy, which answers the question. Logic handles prior X_tsne/y_sub and includes a fallback. Choice of k=3 is reasonable. No issues beyond minor import assumptions.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ You correctly used UMAP embeddings and evaluated KNN accuracy with a proper train/test split. Code aligns with your prior work (X_umap, y_sub) and should run. Nice use of k=3. Consider tuning k or weights for potential gains.

---

### Exercise 4 (17/20 = 85.0%)

**Part ex2-part1** (ex2-part1.code): 7/7 points

_Feedback:_ Good work. You applied PCA, trained KNN on transformed data, reported accuracy, and provided a 2D scatter plot. Using n_components=2 is a valid alternative to variance-based selection. Approach is correct and complete for the task.

**Part ex2-part2** (ex2-part2.code): 7/7 points

_Feedback:_ Excellent work. You correctly applied UMAP on the training data, transformed the test set, trained KNN, reported accuracy, and visualized the 2D embedding. Approach aligns with your prior PCA workflow and the task’s goals.

**Part ex2-part3** (ex2-part3.answer): 3/6 points

_Feedback:_ Good comparison: you correctly state UMAP slightly outperforms PCA in 2D and explain linear vs nonlinear/local structure tradeoffs. However, you didn’t explore/compare parameter settings, nor mention that lower nearest neighbors often make UMAP work better in low dims.

---

### Exercise 1 (20/20 = 100.0%)

**Part pipeline-part1** (pipeline-part1.code): 4/4 points

_Feedback:_ Good job. You reduced to 2D with PCA and produced a clear scatter plot colored by class, with labels and colorbar. Printing explained variance is a nice addition. Meets the task requirements.

**Part pipeline-part2** (pipeline-part2.code): 4/4 points

_Feedback:_ Great job. You fit PCA with 40 components, computed percent variance explained, and plotted a proper scree plot for the first 40 PCs with clear labels. This meets the requirements fully.

**Part pipeline-part3** (pipeline-part3.code): 4/4 points

_Feedback:_ Correct and complete. You fit PCA on the training set, computed cumulative explained variance, found the smallest number of components reaching 95%, and visualized it with helpful threshold lines. This addresses the task well.

**Part pipeline-part4** (pipeline-part4.code): 4/4 points

_Feedback:_ Excellent. You used n_components_95 from Step 4, fit PCA on the train set, transformed a test digit, and reconstructed it for visualization. Plotting both original and reconstructed digits clearly meets the objective.

**Part pipeline-part5** (pipeline-part5.code): 4/4 points

_Feedback:_ Well done. You compare KNN with and without PCA and correctly preserve ≥80% variance by selecting components via cumulative variance. Clear reporting of dims, accuracy, and timing. Approach is sound and consistent with the task.

---

## Additional Information

This feedback was automatically generated by the autograder using LLM-based evaluation.

**Generated:** 2025-11-11 17:33:47 UTC

If you have questions about your grade, please reach out to the instructor.

---

*Powered by [Grade-Lite](https://github.com/your-repo/grade-lite) Autograder*